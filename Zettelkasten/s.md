- Input dato da 
	- tutti i dati x
	- Il label set y il training set s
- L'output del learner 
	- restituisce un ipotesi h tale che ad ogni elemento di x sia ssociato un laberl dal label set
	- si basa sulla hs che è uguale all' l'ipotesi resituita dall'algoritmo di apprendimento A dato il training set S 
- Questi dati vanno generati (ignoti)
	- tramite una distribuzione di probabilità D (>![Z-Test](https://i.imgur.com/lr4H8B5.png)) nell'insieme X dei dati in input
	- Funzione di etichettatura f tale che ad ogni x venga associata l'etichetta reale y
- Visto che la funzione f non ci è nota dobbiamo cercare di costruire un algoritmo A che riestituisca un'ipotesi h quanto più vicina ad f possibile
	- Se conoscessimo la distribuzione potremmo misurare l'errore di generalizzazione LD(h) come la probabilità che la funzione h sul punto x sia uguale alla funzione f sullo steso punto
	- Visto che non abbiamo né la distribuzione D né la funzione f dobbiamo usare un approccio empirico
- Si passa quindi dal rischio/errore/loss sulla distribuzione d al rischio empirico Ls(h)
	- Invece di usare la distribuzione D ci appoggiamo ad un training set S di cardinalità m di cui invece conosciamo informazioni
	- Il rischio empirico calcola quindi la frazione di S su cui h si sbaglia 
	- L'obiettivo è quello di stimare LD tramite LS
	- Di conseguenza hS sarà l'ipotesi che minimizza il rischio empirico (argmin LS(h))
		- Esempio: angurie e overfitting
	- Piuttosto che cercare l'ipotesi perfetta identifichiamo una classe di ipotesi H (tutte le ipotesi formate da rettangoli con i lati paralleli agli assi)
		- IN questo caso argmin di h appartenente alla calsse delle ipotesi
		- Ogni ipotesi appartenente ad H classifica un frutto nel rettangolo e uno fuori
		- In questo caso però possiamo avere degli errori, come facciamo a garantire che essi siano limitati?
			- Dobbiamo introdurre due nuovi parametri
				- Accuratezza
					- Errore massimo che ci aspettiamo di ottenere (su un range di 10 valori ad esempio se al posto di dire 5 dico 10 l'errore ottenuto è 5 e posso limitarlo a 3 escludendo le ipotesi che mi portano a valori superiori a 8 o inferiori a 2)
				- Confidenza
					- Percentuale delle volte che mi aspetto di fallire
					- Viene espresso in funzione dell'accuratezza
						- La percentuale di volte in cui l'errore sul dominio risulta minore dell'accuratezza deve essere maggiore di 1 meno la confidenza
						- Essenzialmente do per assodato che si facciano 1-confidenza errori
		- Affinché la formula della confidenza valga devono a loro volta valere delle ipotesi
			- Assunzione di realizzabilità
				- esiste unìipotesi in H h formisce errore sul dominio pari a zero
			- Assunzione di indipendenza
				- I.I.D
			- La classe di ipotesi è finita
				- Se infinita possiamo comunque ridurla per discretizzazione [[I dati#Discretizzazione (1) e binarizzazione (2)]]
			- $m\gt \frac{log{\frac{|H|}{\delta}}}{\epsilon}$
				- Se la cardinalità di H aumenta m deve aumentare
					- Maggiore è il numero di ipotesi che posso "selezionare" maggiore è la probabilità che io trovi quella che porta ad overfitting quindi mi servono più elementi
				- Se la confidenza diminuisce m deve aumentare
					- Se divento più accurato per evitare overfitting devo valutare più elementi
				- Se l'accuratezza diminusice (errore massimo che speriamo di ottenere) m deve aumentare
- Nella realtà gli elementi di X possono presentare attributi/feature che portano ad una classificazione multipla (l'elemento ha più label/etichette)
	- In questo caso non ci interessa più assegnare una etichetta ma capire quale sia la probabilità che l'oggetto  abbia l'etichettay
		- Crolla l'assunzione di realizzabiltà (giustamente, non puoi etichettare con certezza se puoi avere più etichette)
	- L'ipotesi con errore di generalizzazione minimo è quella del Clasificatore Bayesiano [[Classificazione - Tecniche Alternative#Applicazioni del teorema di Bayes alla classificazione]]
		- Formula
	- Agnostic pac learning
		- Problema rilassato del pac learning in cui non esiste più una funzione f quindi piuttosto che avvicinarci a questa funzione valutiamo il rischio bayesiano ovvero il minimo errore possibile (formula)